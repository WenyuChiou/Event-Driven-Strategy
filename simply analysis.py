#%%
import pandas as pd
import matplotlib.pyplot as plt

def calculate_tradingview_kama(prices, period=10, fast_period=2, slow_period=30):
    """
    Calculate KAMA (Kaufman's Adaptive Moving Average) using TradingView-like logic.
    :param prices: Series of closing prices
    :param period: Look-back period for ER calculation
    :param fast_period: Fast smoothing period for SC
    :param slow_period: Slow smoothing period for SC
    :return: Series of KAMA values
    """
    kama = [prices.iloc[0]]  # Initialize KAMA with the first price value
    for t in range(period, len(prices)):
        # Calculate Change and Volatility
        change = abs(prices.iloc[t] - prices.iloc[t - period])
        volatility = sum(abs(prices.iloc[i] - prices.iloc[i - 1]) for i in range(t - period + 1, t + 1))
        # Efficiency Ratio (ER)
        er = change / (volatility + 1e-10)  # Avoid division by zero
        # Smoothing constant (SC)
        sc = (er * (2 / (fast_period + 1)) + (1 - er) * (2 / (slow_period + 1))) ** 2
        # Calculate KAMA
        kama_value = kama[-1] + sc * (prices.iloc[t] - kama[-1])
        kama.append(kama_value)
    # Fill the initial KAMA values with None for alignment
    kama = [None] * (period - 1) + kama
    return pd.Series(kama[:len(prices)], index=prices.index)

# Load the dataset
data = pd.read_excel(r"C:\Users\wenyu\Desktop\trade\investment\python\API\test\TX00\TX00_1_20240924_20241124.xlsx")
data['date'] = pd.to_datetime(data['date'])
data.set_index('date', inplace=True)

# Calculate Short-KAMA using a period of 5
short_period = 5
data['Short_KAMA'] = calculate_tradingview_kama(data['close'], period=short_period)

# Subset data for visualization
subset_length = 100  # Define the number of data points to display
data_subset = data.iloc[:subset_length]

# Plot K-line with Short-KAMA
fig, ax = plt.subplots(figsize=(14, 8))

# Draw K-line (candlestick chart)
for i in range(len(data_subset)):
    color = 'green' if data_subset['close'].iloc[i] >= data_subset['open'].iloc[i] else 'red'
    ax.plot([data_subset.index[i], data_subset.index[i]], 
            [data_subset['low'].iloc[i], data_subset['high'].iloc[i]], color=color, linewidth=0.8)
    ax.plot([data_subset.index[i], data_subset.index[i]], 
            [data_subset['open'].iloc[i], data_subset['close'].iloc[i]], color=color, linewidth=4)

# Plot Short-KAMA
ax.plot(data_subset.index, data_subset['Short_KAMA'], color='blue', linestyle='-', label='Short-KAMA (Window = 5)')

ax.set_title('K-Line with Short-KAMA')
ax.set_xlabel('Datetime')
ax.set_ylabel('Price')
ax.legend()
ax.grid()
plt.show()
#%%
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import numpy as np
import talib


# Bollinger Bands è¨ˆç®—å‡½æ•¸
def calculate_bbands(data, window=20, num_std=2):
    """è¨ˆç®— Bollinger Bands"""
    data['Middle_Band'] = data['close'].rolling(window=window).mean()
    data['Std_Dev'] = data['close'].rolling(window=window).std()
    data['Upper_Band'] = data['Middle_Band'] + num_std * data['Std_Dev']
    data['Lower_Band'] = data['Middle_Band'] - num_std * data['Std_Dev']
    return data


# MACD è¨ˆç®—å‡½æ•¸
def calculate_macd(data, short=12, long=26, signal=9):
    """è¨ˆç®— MACD å’ŒæŸ±é«”"""
    data['EMA_short'] = data['close'].ewm(span=short).mean()
    data['EMA_long'] = data['close'].ewm(span=long).mean()
    data['MACD'] = data['EMA_short'] - data['EMA_long']
    data['MACD_Hist'] = data['MACD'] - data['MACD'].ewm(span=signal).mean()
    return data


def detect_accelerated_downtrend_events_with_profit_loss(
    data, slope_window=5, ema_window=5, avg_vol_window=9, profit_loss_window=5, long_ema_window=20
):
    """
    Detect events based on specified criteria:
    1. Downtrend (negative slope of Lower_Band).
    2. Accelerated downtrend (slope change becomes steeper).
    3. Rebound above EMA(3).
    4. Profit/Loss points > 3x average volatility of past 9 windows.
    5. 20-day EMA is trending downward.
    """
    # Step 1: Calculate rolling slope of Lower_Band over slope_window
    data['Lower_Band_Slope'] = data['Lower_Band'].diff(slope_window) / data['Lower_Band'].shift(slope_window)
    data['Lower_Band_Slope'] = data['Lower_Band_Slope'].fillna(0)

    # Normalize slope to [-1, 1]
    scaler = MinMaxScaler(feature_range=(-1, 1))
    data['Normalized_Slope'] = scaler.fit_transform(data[['Lower_Band_Slope']])

    # Step 2: Calculate slope change over slope_window
    data['Slope_Change'] = data['Normalized_Slope'] - data['Normalized_Slope'].shift(slope_window)
    data['Accelerated_Downtrend'] = (data['Lower_Band_Slope'] < 0) & (data['Slope_Change'] < 0)

    # Step 3: Calculate EMA(3)
    data['EMA'] = talib.EMA(data['close'], timeperiod=ema_window)
    data['Rebound_Above_EMA'] = data['close'] > data['EMA']

    # Step 4: Calculate long-term EMA (20-day EMA)
    data['Long_EMA'] = talib.EMA(data['close'], timeperiod=long_ema_window)
    data['Long_EMA_Downward'] = data['Long_EMA'].diff() < 0

    # Step 5: Calculate average volatility
    data['Average_Volatility'] = data['close'].diff().abs().rolling(window=avg_vol_window).mean()

    # Step 6: Calculate profit/loss points
    data['Profit_Loss_Points'] = data['close'].shift(-profit_loss_window) - data['close']

    # Step 7: Identify events
    data['Event'] = (
        (data['Lower_Band_Slope'] < 0)
        & data['Accelerated_Downtrend']
        & data['Rebound_Above_EMA']
        & (data['Profit_Loss_Points'] > 3 * data['Average_Volatility'])
        & (data['Profit_Loss_Points'] > 10)
        & data['Long_EMA_Downward']
    )

    return data, scaler


# Main script
file_path = r"C:\Users\wenyu\Desktop\trade\investment\python\API\test\TX00\TX00_1_20240924_20241124.xlsx"

# Load and prepare data
df = pd.ExcelFile(file_path).parse('Sheet1')
df['date'] = pd.to_datetime(df['date'])
df = df.sort_values(by='date').reset_index(drop=True)

# Calculate indicators
df = calculate_macd(df)
df = calculate_bbands(df)

# Detect events
df, scaler = detect_accelerated_downtrend_events_with_profit_loss(df, profit_loss_window=5)
# æ ‡æ³¨æ­£æ ·æœ¬ï¼šæ»¡è¶³æ¡ä»¶çš„äº‹ä»¶
df['Label'] = 0  # åˆå§‹åŒ–ä¸ºè´Ÿæ ·æœ¬
df.loc[df['Event'] & (df['Profit_Loss_Points'] > 0), 'Label'] = 1

# Drop the 'Profit_Loss_Points' column if it exists
if 'Profit_Loss_Points' in df.columns:
    df = df.drop(columns=['Profit_Loss_Points'])
# Filter detected events
detected_events = df[df['Event']]

# Print event information
print(detected_events[['close', 'Lower_Band_Slope', 'Slope_Change', 'EMA', 'Average_Volatility']])
print(f"Total events detected: {len(detected_events)}")
print(f"percentage of positive events: {len(detected_events[detected_events['Label'] == 1]) / len(df)*100} %")




# %%
import matplotlib.pyplot as plt

def plot_kline_with_signals(data):
    """
    Plot K-line chart with detected event signals and EMA.
    
    :param data: DataFrame containing close prices, EMA, Long_EMA, and Event information.
    """
    plt.figure(figsize=(16, 10))
    
    # ç»˜åˆ¶ K çº¿å›¾ï¼ˆç”¨ close ä»·æ ¼ä»£æ›¿ç®€åŒ–ç‰ˆæœ¬ï¼‰
    plt.plot(data.index, data['close'], label='Close Price', color='blue', linewidth=1.5)
    
    # ç»˜åˆ¶çŸ­æœŸ EMA
    plt.plot(data.index, data['EMA'], label='EMA (5)', color='purple', linestyle='--', linewidth=1.5)
    
    # ç»˜åˆ¶é•¿æœŸ EMA
    plt.plot(data.index, data['Long_EMA'], label='Long EMA (20)', color='orange', linestyle='-', linewidth=1.5)
    
    # æ ‡è®°äº‹ä»¶å‘ç”Ÿçš„ä½ç½®
    event_data = data[data['Event']]  # ç­›é€‰å‡ºæ ‡è®°ä¸ºäº‹ä»¶çš„è¡Œ
    plt.scatter(event_data.index, event_data['close'], color='red', label='Detected Events', zorder=5)
    
    # æ·»åŠ æ ‡é¢˜å’Œå›¾ä¾‹
    plt.title('K-Line Chart with Detected Events and EMA', fontsize=16)
    plt.xlabel('Date', fontsize=14)
    plt.ylabel('Price', fontsize=14)
    plt.legend(fontsize=12)
    
    # æ˜¾ç¤ºå›¾è¡¨
    plt.grid()
    plt.show()

# ç»˜åˆ¶å›¾è¡¨
plot_kline_with_signals(df)
# %%
print(f"Total events detected: {len(detected_events)}")
print(detected_events[['close', 'Average_Volatility']].describe())

# %%
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, roc_auc_score
from sklearn.model_selection import TimeSeriesSplit
from model.ModelLoader import *
import numpy as np

features = ['Lower_Band_Slope', 'Slope_Change', 'Average_Volatility', 'close', 'volume']
X = df[features]
y = df['Label']

model_name = 'catboost'
optimizer = BayesianOptimizer(model_name=model_name)
optimizer.fit(X=X.to_numpy(), y=y.to_numpy(),n_splits=10)
# ç²å–æœ€ä½³åƒæ•¸å’Œæ¬Šé‡
best_params, weights = optimizer.get_best_params_and_weights()
print("æœ€ä½³åƒæ•¸:", best_params)
print("åƒæ•¸æ¬Šé‡:", weights)

# ç²å–ç‰¹å¾µé‡è¦æ€§
feature_importances = optimizer.get_feature_importances()
print("ç‰¹å¾µé‡è¦æ€§:", feature_importances)

import json

besthyperparameter_name = f"best_hyperparams_{model_name}.json"
bestweights_name = f"best_weights_{model_name}.json"
# ä¿å­˜æœ€ä½³è¶…å‚æ•°
with open(besthyperparameter_name, "w") as f:
    json.dump(best_params, f)

# ä¿å­˜æƒé‡
with open(bestweights_name, "w") as f:
    json.dump(weights, f)

#%%

from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from catboost import CatBoostClassifier
from lightgbm import LGBMRegressor
import joblib

# è¯»å–æœ€ä½³è¶…å‚æ•°
with open(besthyperparameter_name, "r") as f:
    best_params = json.load(f)

# è¯»å–æƒé‡
with open(bestweights_name, "r") as f:
    weights = json.load(f)


# train model
# model_RF = RandomForestClassifier(**best_params, random_state=42)
model = CatBoostClassifier(**best_params, random_state=42)
# model_RF.fit(X, y)  # X_train, y_train ä¹‹å‰å·²ç»å®šä¹‰
model.fit(X, y)  # X_train, y_train

# ä¿å­˜æ¨¡å‹åˆ°æœ¬åœ°æ–‡ä»¶
model_filename = f"model_{model_name}.joblib"
joblib.dump(model, model_filename)
joblib.dump(scaler, "scaler.pkl")  # ä¿å­˜æ ‡å‡†åŒ–å™¨
print(f"æ¨¡å‹å·²ä¿å­˜åˆ° {model_filename}")


# %%
import matplotlib.pyplot as plt
feature_importances = model.feature_importances_
plt.bar(X.columns, feature_importances)
plt.xticks(rotation=45)
plt.show()

# %%
import pandas as pd
import talib
from sklearn.preprocessing import MinMaxScaler

# Load real-time data
real_time_file = r"C:\Users\wenyu\Desktop\trade\investment\python\API\test\TX00\TX00_1_20241224_20250117.xlsx"
real_time_data = pd.ExcelFile(real_time_file).parse('Sheet1')

# Ensure 'date' column is datetime and filter real-time data starting from 2025-01-12
real_time_data['date'] = pd.to_datetime(real_time_data['date'])
# real_time_data = real_time_data[real_time_data['date'] >= '2025-01-12']
real_time_data = real_time_data.sort_values(by='date').reset_index(drop=True)

# Feature Engineering: Apply the same logic as in training
def calculate_realtime_features(data, slope_window=5, ema_window=5, avg_vol_window=9, long_ema_window=20, scaler=None):
    """Calculate features for real-time data."""
    # Calculate Bollinger Bands
    data['Middle_Band'] = data['close'].rolling(window=20).mean()
    data['Std_Dev'] = data['close'].rolling(window=20).std()
    data['Lower_Band'] = data['Middle_Band'] - 2 * data['Std_Dev']
    
    # Calculate slopes
    data['Lower_Band_Slope'] = data['Lower_Band'].diff(slope_window) / data['Lower_Band'].shift(slope_window)
    data['Lower_Band_Slope'] = data['Lower_Band_Slope'].fillna(0)


    if len(data) >= slope_window:  # Apply scaler only when enough data exists
        data['Normalized_Slope'] = scaler.fit_transform(data[['Lower_Band_Slope']])
    else:
        data['Normalized_Slope'] = 0

    # Slope change
    data['Slope_Change'] = data['Normalized_Slope'] - data['Normalized_Slope'].shift(slope_window)
    data['Slope_Change'] = data['Slope_Change'].fillna(0)

    # EMA calculations
    data['EMA'] = talib.EMA(data['close'], timeperiod=ema_window)
    data['Long_EMA'] = talib.EMA(data['close'], timeperiod=long_ema_window)

    # Average volatility
    data['Average_Volatility'] = data['close'].diff().abs().rolling(window=avg_vol_window).mean()

    return data

# Apply feature engineering to real-time data
real_time_data = calculate_realtime_features(real_time_data, scaler=scaler)

# Prepare features for prediction (ensure columns match training features)
X_real_time = real_time_data[features].fillna(0)  # Replace NaNs with 0 for real-time data

# Predict probabilities
real_time_data['Event_Probability'] = model.predict_proba(X_real_time)[:, 1]

# Step 6: Calculate profit/loss points
real_time_data['Profit_Loss_Points'] = real_time_data['close'].shift(-5) - real_time_data['close']

# Filter and display high-probability events
probability_threshold = 0.010  # Threshold for event detection
high_probability_events = real_time_data[real_time_data['Event_Probability'] >= probability_threshold]

# Output high-probability events
print("Real-Time Event Predictions:")
# print(high_probability_events[['date', 'close', 'Event_Probability','Profit_Loss_Points']])
print(f"Total Profit/Loss Points: {high_probability_events['Profit_Loss_Points'].sum()}")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# ç¢ºä¿ real_time_data æœ‰éœ€è¦çš„æ¬„ä½
if 'Profit_Loss_Points' not in real_time_data.columns:
    real_time_data['Profit_Loss_Points'] = real_time_data['close'].shift(-5) - real_time_data['close']

def strategy_metrics(trade_pnl):
    """è®¡ç®—ç­–ç•¥è¯„ä¼°æŒ‡æ ‡ï¼ˆä¿®æ­£ Sharpe Ratio è®¡ç®—æ–¹å¼ï¼‰"""
    total_pnl = trade_pnl.sum()  # æ€»ç›ˆäº
    win_trades = trade_pnl[trade_pnl > 0]
    loss_trades = trade_pnl[trade_pnl < 0]

    # ç›ˆäºæ¯” (Profit Factor)
    profit_factor = win_trades.sum() / abs(loss_trades.sum()) if abs(loss_trades.sum()) > 0 else np.nan

    # èƒœç‡ (Win Rate)
    win_rate = len(win_trades) / len(trade_pnl) if len(trade_pnl) > 0 else np.nan

    # æœŸæœ›ç›ˆäº (Expectancy)
    expectancy = (win_trades.mean() * win_rate) - (loss_trades.mean() * (1 - win_rate))

    # **ä¿®æ­£ Sharpe Ratio è®¡ç®—**
    if trade_pnl.std() > 0:
        annualized_factor = np.sqrt(len(trade_pnl))  # äº¤æ˜“æ¬¡æ•°å†³å®šå¹´åŒ–å› å­
        sharpe_ratio = (trade_pnl.mean() / trade_pnl.std()) * annualized_factor
    else:
        sharpe_ratio = np.nan  # é¿å…é™¤é›¶é”™è¯¯

    # æœ€å¤§å›æ’¤ (Max Drawdown)
    cumulative_pnl = trade_pnl.cumsum()
    running_max = np.maximum.accumulate(cumulative_pnl)
    drawdown = running_max - cumulative_pnl
    max_drawdown = drawdown.max()

    return {
        'Total PnL': total_pnl,
        'Profit Factor': profit_factor,
        'Win Rate': win_rate,
        'Expectancy': expectancy,
        'Sharpe Ratio': sharpe_ratio,  # ç°åœ¨çš„ Sharpe Ratio ä¸ä¼šå¼‚å¸¸é«˜
        'Max Drawdown': max_drawdown,
    }

# éæ­·ä¸åŒçš„æ©Ÿç‡é–¾å€¼ä¾†æ‰¾æœ€ä½³å€¼
# RF: best_threshold = 0.015, best_sharpe = 1.5
# CatBoost: best_threshold = 0.159, best_sharpe = 1.5

probability_thresholds = np.arange(0.17, 0.181, 0.001)
best_threshold = None
best_sharpe = -np.inf
results = []
cumulative_pnl_dict = {}  # å„²å­˜ä¸åŒé–¥å€¼çš„ç´¯ç©ç›ˆè™§

for threshold in probability_thresholds:
    selected_trades = real_time_data[real_time_data['Event_Probability'] >= threshold]['Profit_Loss_Points']
    
    if len(selected_trades) > 0:
        metrics = strategy_metrics(selected_trades)
        metrics['Threshold'] = threshold
        results.append(metrics)

        # å„²å­˜ç´¯ç©æ”¶ç›Š
        cumulative_pnl_dict[threshold] = selected_trades.cumsum()

        # å„ªå…ˆé¸æ“‡æœ€å¤§å¤æ™®æ¯”ç‡ æˆ– ç›ˆè™§æ¯” > 1.5 ä¸”å›æ’¤æœ€å°
        if metrics['Sharpe Ratio'] > best_sharpe and metrics['Profit Factor'] > 1.5:
            best_sharpe = metrics['Sharpe Ratio']

# å»ºç«‹çµæœ DataFrame
results_df = pd.DataFrame(results)
print(results_df)

# ç¹ªè£½ä¸åŒé–¥å€¼çš„ç´¯ç©æ”¶ç›Šæ›²ç·š
plt.figure(figsize=(12, 6))
for threshold, pnl in cumulative_pnl_dict.items():
    plt.plot(pnl.index, pnl, label=f'Threshold {threshold:.3f}')

plt.xlabel("Time")
plt.ylabel("Cumulative Profit/Loss")
plt.title("Cumulative PnL for Different Probability Thresholds")
plt.legend()
plt.grid()
plt.show()

# ç¹ªè£½æœ€ä½³é–¥å€¼çš„ç´¯ç©æ”¶ç›Šæ›²ç·š
if best_threshold is not None:
    plt.figure(figsize=(12, 6))
    plt.plot(cumulative_pnl_dict[best_threshold].index, cumulative_pnl_dict[best_threshold], label=f'Best Threshold: {best_threshold:.3f}', color='red', linewidth=2)
    
    plt.xlabel("Time")
    plt.ylabel("Cumulative Profit/Loss")
    plt.title("Best Probability Threshold - Cumulative PnL")
    plt.legend()
    plt.grid()
    plt.show()

    print(f"\næœ€ä½³é–¥å€¼: {best_threshold}")
else:
    print("æ²’æœ‰æ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„æœ€ä½³é–¥å€¼ã€‚")



# %%
# %%
import pandas as pd
import time
import joblib
import talib
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

# **1ï¸âƒ£ è½½å…¥æ¨¡å‹ & Scaler**
# model = joblib.load("model_random_forest.joblib")  # è½½å…¥å·²è®­ç»ƒå¥½çš„æ¨¡å‹
# scaler = joblib.load("scaler.pkl")  # è½½å…¥æ ‡å‡†åŒ–å™¨

# **2ï¸âƒ£ è¯»å– Real-Time Data**
real_time_file = r"C:\Users\wenyu\Desktop\trade\investment\python\API\test\TX00\TX00_1_20241224_20250117.xlsx"
real_time_data = pd.ExcelFile(real_time_file).parse('Sheet1')
real_time_data['date'] = pd.to_datetime(real_time_data['date'])
# real_time_data = real_time_data[real_time_data['date'] >= '2025-01-12']
real_time_data = real_time_data.sort_values(by='date').reset_index(drop=True)

# **3ï¸âƒ£ è®¾å®šå‚æ•°**
features = ['Lower_Band_Slope', 'Slope_Change', 'Average_Volatility', 'close', 'volume']
probability_threshold = 0.015  # äº¤æ˜“ä¿¡å·æ¦‚ç‡é˜ˆå€¼
holding_period = pd.Timedelta(minutes=5)  # **æŒä»“ 5 åˆ†é’Ÿ**
last_trade_time = None  # **è®°å½•ä¸Šæ¬¡äº¤æ˜“æ—¶é—´**

# åˆå§‹åŒ–äº¤æ˜“è¨˜éŒ„
trade_records = []

# **4ï¸âƒ£ å®šä¹‰è®¡ç®—ç‰¹å¾çš„å‡½æ•°**
def calculate_realtime_features(data, scaler, slope_window=5, ema_window=5, avg_vol_window=9, long_ema_window=20):
    """è®¡ç®—å®æ—¶æ•°æ®çš„ç‰¹å¾"""
    data['Middle_Band'] = data['close'].rolling(window=20).mean()
    data['Std_Dev'] = data['close'].rolling(window=20).std()
    data['Lower_Band'] = data['Middle_Band'] - 2 * data['Std_Dev']

    # è®¡ç®— Lower_Band æ–œç‡
    data['Lower_Band_Slope'] = data['Lower_Band'].diff(slope_window) / data['Lower_Band'].shift(slope_window)
    data['Lower_Band_Slope'] = data['Lower_Band_Slope'].fillna(0)

    # æ–œç‡æ ‡å‡†åŒ–
    if len(data) >= slope_window:
        data['Normalized_Slope'] = scaler.transform(data[['Lower_Band_Slope']])
    else:
        data['Normalized_Slope'] = 0

    # è®¡ç®—æ–œç‡å˜åŒ–
    data['Slope_Change'] = data['Normalized_Slope'] - data['Normalized_Slope'].shift(slope_window)
    data['Slope_Change'] = data['Slope_Change'].fillna(0)

    # è®¡ç®—çŸ­æœŸ EMA å’Œ é•¿æœŸ EMA
    data['EMA'] = talib.EMA(data['close'], timeperiod=ema_window)
    data['Long_EMA'] = talib.EMA(data['close'], timeperiod=long_ema_window)

    # è®¡ç®—å¹³å‡æ³¢åŠ¨ç‡
    data['Average_Volatility'] = data['close'].diff().abs().rolling(window=avg_vol_window).mean()

    return data

def strategy_metrics(trade_pnl):
    """è®¡ç®—ç­–ç•¥è¯„ä¼°æŒ‡æ ‡ï¼ˆä¿®æ­£ Sharpe Ratio è®¡ç®—æ–¹å¼ï¼‰"""
    total_pnl = trade_pnl.sum()  # æ€»ç›ˆäº
    win_trades = trade_pnl[trade_pnl > 0]
    loss_trades = trade_pnl[trade_pnl < 0]

    # ç›ˆäºæ¯” (Profit Factor)
    profit_factor = win_trades.sum() / abs(loss_trades.sum()) if abs(loss_trades.sum()) > 0 else np.nan

    # èƒœç‡ (Win Rate)
    win_rate = len(win_trades) / len(trade_pnl) if len(trade_pnl) > 0 else np.nan

    # æœŸæœ›ç›ˆäº (Expectancy)
    expectancy = (win_trades.mean() * win_rate) - (loss_trades.mean() * (1 - win_rate))

    # **ä¿®æ­£ Sharpe Ratio è®¡ç®—**
    if trade_pnl.std() > 0:
        annualized_factor = np.sqrt(len(trade_pnl))  # äº¤æ˜“æ¬¡æ•°å†³å®šå¹´åŒ–å› å­
        sharpe_ratio = (trade_pnl.mean() / trade_pnl.std()) * annualized_factor
    else:
        sharpe_ratio = np.nan  # é¿å…é™¤é›¶é”™è¯¯

    # æœ€å¤§å›æ’¤ (Max Drawdown)
    cumulative_pnl = trade_pnl.cumsum()
    running_max = np.maximum.accumulate(cumulative_pnl)
    drawdown = running_max - cumulative_pnl
    max_drawdown = drawdown.max()

    return {
        'Total PnL': total_pnl,
        'Profit Factor': profit_factor,
        'Win Rate': win_rate,
        'Expectancy': expectancy,
        'Sharpe Ratio': sharpe_ratio,  # ç°åœ¨çš„ Sharpe Ratio ä¸ä¼šå¼‚å¸¸é«˜
        'Max Drawdown': max_drawdown,
    }



# **5ï¸âƒ£ æ¨¡æ‹Ÿå®æ—¶é€æ­¥æ•°æ®**
for i in range(len(real_time_data)):
    # **æ¨¡æ‹Ÿé€æ­¥è·å–æ•°æ®**
    current_data = real_time_data.iloc[: i + 1].copy()

    # **è®¡ç®—ç‰¹å¾**
    current_data = calculate_realtime_features(current_data, scaler)

    # **æå–å½“å‰æœ€æ–°çš„æ•°æ®ç‚¹**
    latest_row = current_data.iloc[-1:][features].fillna(0)
    current_time = current_data.iloc[-1]['date']

    # **æ£€æŸ¥æ˜¯å¦åœ¨æŒä»“æœŸå†…**
    if last_trade_time is not None and current_time < last_trade_time + holding_period:
        continue  # **5 åˆ†é’Ÿå†…ä¸è¿›è¡Œæ–°äº¤æ˜“**

    # **æ¨¡å‹é¢„æµ‹**
    event_probability = model.predict_proba(latest_row)[:, 1][0]

    # **æ£€æŸ¥æ˜¯å¦è¾¾åˆ°äº¤æ˜“ä¿¡å·**
    if event_probability >= probability_threshold:
        # **è®¡ç®—ç›ˆäºç‚¹æ•°**
        if i + 5 < len(real_time_data):
            profit_loss_points = real_time_data.iloc[i + 5]['close'] - real_time_data.iloc[i]['close']
        else:
            profit_loss_points = 0  # è‹¥æœªæ¥æ•°æ®ä¸è¶³ï¼Œåˆ™ä¸è®¡ç®—ç›ˆäº

        # **è®°å½•äº¤æ˜“**
        trade_records.append({
            "date": current_time,
            "close": current_data.iloc[-1]['close'],
            "event_probability": event_probability,
            "profit_loss_points": profit_loss_points
        })

        # **æ›´æ–°æœ€è¿‘äº¤æ˜“æ—¶é—´**
        last_trade_time = current_time

        print(f"ğŸ“Œ äº¤æ˜“ä¿¡å· - æ—¶é—´: {current_time}, ä»·æ ¼: {current_data.iloc[-1]['close']:.2f}, äº‹ä»¶æ¦‚ç‡: {event_probability:.4f}, ç›ˆäº: {profit_loss_points:.2f}")


# **6ï¸âƒ£ è®¡ç®—ç´¯ç§¯ç›ˆäºå¹¶ç»˜åˆ¶**
trade_df = pd.DataFrame(trade_records)
trade_df['cumulative_pnl'] = trade_df['profit_loss_points'].cumsum()

# è®¡ç®—ç­–ç•¥æŒ‡æ ‡
metrics = strategy_metrics(trade_df['profit_loss_points'])
print("ç­–ç•¥è¯„ä¼°æŒ‡æ ‡:", metrics)

# **ç»˜åˆ¶ K çº¿å›¾ + ç´¯ç§¯ç›ˆäºæ›²çº¿**
fig, ax1 = plt.subplots(figsize=(12, 6))

# ç´¯ç§¯ç›ˆäºæ›²çº¿
ax1.plot(trade_df['date'], trade_df['cumulative_pnl'], color='green', label="Cumulative PnL")
ax1.set_xlabel("Date")
ax1.set_ylabel("Cumulative PnL", color="green")
ax1.tick_params(axis="y", labelcolor="green")

# äº¤æ˜“ä¿¡å·ç‚¹
ax1.scatter(trade_df['date'], trade_df['cumulative_pnl'], zorder=5)

# æ ‡é¢˜å’Œå›¾ä¾‹
plt.title("Real-Time Cumulative PnL with Trade Signals")
ax1.legend()

plt.grid()
plt.show()


# %%
# CatBoost: best_threshold = 0.159, best_sharpe = 
# ç­–ç•¥è¯„ä¼°æŒ‡æ ‡: {'Total PnL': 1172, 'Profit Factor': 1.35742604452577, 'Win Rate': 0.5219206680584552, 'Expectancy': 16.617612627553505, 'Sharpe Ratio': 1.9316306659907083, 'Max Drawdown': 243}
